{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xCEmUZ7xo3xUUxK4q9nAmMdoGHYkmqaZ","timestamp":1761830338034},{"file_id":"1LrTccvw6mV-04eQLyGwjUhZXYffuUtsJ","timestamp":1612659040535},{"file_id":"1-dnzR2VxhCok9MsuJ41HP5ss3e_hLixI","timestamp":1610539093559},{"file_id":"11b6niN9PGfwrSGR56TMLvvg74V8tLuBO","timestamp":1610465156105}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vfADtNY3BDz2"},"source":["# Sample data prep"]},{"cell_type":"code","metadata":{"id":"ah7zV0rGBIg8"},"source":["#for demonstration purpose, we take the abstract from a pubmed article\n","#(PMID: 30443819) and text processing\n","\n","document = \"The curation of neuroscience entities is crucial to ongoing efforts in neuroinformatics and computational neuroscience, such as those being deployed in the context of continuing large-scale brain modelling projects. However, manually sifting through thousands of articles for new information about modelled entities is a painstaking and low-reward task. Text mining can be used to help a curator extract relevant information from this literature in a systematic way. We propose the application of text mining methods for the neuroscience literature. Specifically, two computational neuroscientists annotated a corpus of entities pertinent to neuroscience using active learning techniques to enable swift, targeted annotation. We then trained machine learning models to recognise the entities that have been identified. The entities covered are Neuron Types, Brain Regions, Experimental Values, Units, Ion Currents, Channels, and Conductances and Model organisms. We tested a traditional rule-based approach, a conditional random field and a model using deep learning named entity recognition, finding that the deep learning model was superior. Our final results show that we can detect a range of named entities of interest to the neuroscientist with a macro average precision, recall and F1 score of 0.866, 0.817 and 0.837 respectively. The contributions of this work are as follows: 1) We provide a set of Named Entity Recognition (NER) tools that are capable of detecting neuroscience entities with performance above or similar to prior work. 2) We propose a methodology for training NER tools for neuroscience that requires very little training data to get strong performance. This can be adapted for any sub-domain within neuroscience. 3) We provide a small corpus with annotations for multiple entity types, as well as annotation guidelines to help others reproduce our experiments.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WycFSrL__nef","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761830920039,"user_tz":240,"elapsed":13,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"ba4fd04e-a93c-4bbc-b748-ab6237d9cc3c"},"source":["print(document)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The curation of neuroscience entities is crucial to ongoing efforts in neuroinformatics and computational neuroscience, such as those being deployed in the context of continuing large-scale brain modelling projects. However, manually sifting through thousands of articles for new information about modelled entities is a painstaking and low-reward task. Text mining can be used to help a curator extract relevant information from this literature in a systematic way. We propose the application of text mining methods for the neuroscience literature. Specifically, two computational neuroscientists annotated a corpus of entities pertinent to neuroscience using active learning techniques to enable swift, targeted annotation. We then trained machine learning models to recognise the entities that have been identified. The entities covered are Neuron Types, Brain Regions, Experimental Values, Units, Ion Currents, Channels, and Conductances and Model organisms. We tested a traditional rule-based approach, a conditional random field and a model using deep learning named entity recognition, finding that the deep learning model was superior. Our final results show that we can detect a range of named entities of interest to the neuroscientist with a macro average precision, recall and F1 score of 0.866, 0.817 and 0.837 respectively. The contributions of this work are as follows: 1) We provide a set of Named Entity Recognition (NER) tools that are capable of detecting neuroscience entities with performance above or similar to prior work. 2) We propose a methodology for training NER tools for neuroscience that requires very little training data to get strong performance. This can be adapted for any sub-domain within neuroscience. 3) We provide a small corpus with annotations for multiple entity types, as well as annotation guidelines to help others reproduce our experiments.\n"]}]},{"cell_type":"code","metadata":{"id":"FolPNkyIprJL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761830920049,"user_tz":240,"elapsed":11,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"edbf43b1-340e-4bf9-943a-0b9d0bd1cedd"},"source":["print(len(document))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1888\n"]}]},{"cell_type":"markdown","metadata":{"id":"IL4QVlvUbcI6"},"source":["# Import related packages"]},{"cell_type":"code","metadata":{"id":"qa1drhA2ErHQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761830920106,"user_tz":240,"elapsed":55,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"a2335f96-7bc2-4a0a-d043-8d9ee416d1f0"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import wordnet"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"4jyQbqTz6f2_"},"source":["# Case folding"]},{"cell_type":"code","metadata":{"id":"umONPAFj7xNH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761830920142,"user_tz":240,"elapsed":42,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"0eb4d1d5-ee56-4ce9-bfe7-885c21708b26"},"source":["document_lower = document.lower()\n","print(document_lower)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the curation of neuroscience entities is crucial to ongoing efforts in neuroinformatics and computational neuroscience, such as those being deployed in the context of continuing large-scale brain modelling projects. however, manually sifting through thousands of articles for new information about modelled entities is a painstaking and low-reward task. text mining can be used to help a curator extract relevant information from this literature in a systematic way. we propose the application of text mining methods for the neuroscience literature. specifically, two computational neuroscientists annotated a corpus of entities pertinent to neuroscience using active learning techniques to enable swift, targeted annotation. we then trained machine learning models to recognise the entities that have been identified. the entities covered are neuron types, brain regions, experimental values, units, ion currents, channels, and conductances and model organisms. we tested a traditional rule-based approach, a conditional random field and a model using deep learning named entity recognition, finding that the deep learning model was superior. our final results show that we can detect a range of named entities of interest to the neuroscientist with a macro average precision, recall and f1 score of 0.866, 0.817 and 0.837 respectively. the contributions of this work are as follows: 1) we provide a set of named entity recognition (ner) tools that are capable of detecting neuroscience entities with performance above or similar to prior work. 2) we propose a methodology for training ner tools for neuroscience that requires very little training data to get strong performance. this can be adapted for any sub-domain within neuroscience. 3) we provide a small corpus with annotations for multiple entity types, as well as annotation guidelines to help others reproduce our experiments.\n"]}]},{"cell_type":"code","metadata":{"id":"DYALEmkJBmCu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761830920143,"user_tz":240,"elapsed":15,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"895563cc-8e81-4eb8-e868-a8ed191b9752"},"source":["document_upper = document.upper()\n","print(document_upper)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["THE CURATION OF NEUROSCIENCE ENTITIES IS CRUCIAL TO ONGOING EFFORTS IN NEUROINFORMATICS AND COMPUTATIONAL NEUROSCIENCE, SUCH AS THOSE BEING DEPLOYED IN THE CONTEXT OF CONTINUING LARGE-SCALE BRAIN MODELLING PROJECTS. HOWEVER, MANUALLY SIFTING THROUGH THOUSANDS OF ARTICLES FOR NEW INFORMATION ABOUT MODELLED ENTITIES IS A PAINSTAKING AND LOW-REWARD TASK. TEXT MINING CAN BE USED TO HELP A CURATOR EXTRACT RELEVANT INFORMATION FROM THIS LITERATURE IN A SYSTEMATIC WAY. WE PROPOSE THE APPLICATION OF TEXT MINING METHODS FOR THE NEUROSCIENCE LITERATURE. SPECIFICALLY, TWO COMPUTATIONAL NEUROSCIENTISTS ANNOTATED A CORPUS OF ENTITIES PERTINENT TO NEUROSCIENCE USING ACTIVE LEARNING TECHNIQUES TO ENABLE SWIFT, TARGETED ANNOTATION. WE THEN TRAINED MACHINE LEARNING MODELS TO RECOGNISE THE ENTITIES THAT HAVE BEEN IDENTIFIED. THE ENTITIES COVERED ARE NEURON TYPES, BRAIN REGIONS, EXPERIMENTAL VALUES, UNITS, ION CURRENTS, CHANNELS, AND CONDUCTANCES AND MODEL ORGANISMS. WE TESTED A TRADITIONAL RULE-BASED APPROACH, A CONDITIONAL RANDOM FIELD AND A MODEL USING DEEP LEARNING NAMED ENTITY RECOGNITION, FINDING THAT THE DEEP LEARNING MODEL WAS SUPERIOR. OUR FINAL RESULTS SHOW THAT WE CAN DETECT A RANGE OF NAMED ENTITIES OF INTEREST TO THE NEUROSCIENTIST WITH A MACRO AVERAGE PRECISION, RECALL AND F1 SCORE OF 0.866, 0.817 AND 0.837 RESPECTIVELY. THE CONTRIBUTIONS OF THIS WORK ARE AS FOLLOWS: 1) WE PROVIDE A SET OF NAMED ENTITY RECOGNITION (NER) TOOLS THAT ARE CAPABLE OF DETECTING NEUROSCIENCE ENTITIES WITH PERFORMANCE ABOVE OR SIMILAR TO PRIOR WORK. 2) WE PROPOSE A METHODOLOGY FOR TRAINING NER TOOLS FOR NEUROSCIENCE THAT REQUIRES VERY LITTLE TRAINING DATA TO GET STRONG PERFORMANCE. THIS CAN BE ADAPTED FOR ANY SUB-DOMAIN WITHIN NEUROSCIENCE. 3) WE PROVIDE A SMALL CORPUS WITH ANNOTATIONS FOR MULTIPLE ENTITY TYPES, AS WELL AS ANNOTATION GUIDELINES TO HELP OTHERS REPRODUCE OUR EXPERIMENTS.\n"]}]},{"cell_type":"markdown","metadata":{"id":"R5zIUqxsBvkV"},"source":["Note that you can lower/upper at sentence or word level as well."]},{"cell_type":"markdown","metadata":{"id":"JUSPh1HsB8a9"},"source":["# Word tokenization"]},{"cell_type":"code","metadata":{"id":"DVSpxfFmDkS9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761830920144,"user_tz":240,"elapsed":8,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"80dd9eab-90cf-4a69-c646-d0e1ba2e4885"},"source":["#a simpe way of word tokenization is just split by space:\n","words_split = document_lower.split()\n","for word in words_split:\n","  print(word)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the\n","curation\n","of\n","neuroscience\n","entities\n","is\n","crucial\n","to\n","ongoing\n","efforts\n","in\n","neuroinformatics\n","and\n","computational\n","neuroscience,\n","such\n","as\n","those\n","being\n","deployed\n","in\n","the\n","context\n","of\n","continuing\n","large-scale\n","brain\n","modelling\n","projects.\n","however,\n","manually\n","sifting\n","through\n","thousands\n","of\n","articles\n","for\n","new\n","information\n","about\n","modelled\n","entities\n","is\n","a\n","painstaking\n","and\n","low-reward\n","task.\n","text\n","mining\n","can\n","be\n","used\n","to\n","help\n","a\n","curator\n","extract\n","relevant\n","information\n","from\n","this\n","literature\n","in\n","a\n","systematic\n","way.\n","we\n","propose\n","the\n","application\n","of\n","text\n","mining\n","methods\n","for\n","the\n","neuroscience\n","literature.\n","specifically,\n","two\n","computational\n","neuroscientists\n","annotated\n","a\n","corpus\n","of\n","entities\n","pertinent\n","to\n","neuroscience\n","using\n","active\n","learning\n","techniques\n","to\n","enable\n","swift,\n","targeted\n","annotation.\n","we\n","then\n","trained\n","machine\n","learning\n","models\n","to\n","recognise\n","the\n","entities\n","that\n","have\n","been\n","identified.\n","the\n","entities\n","covered\n","are\n","neuron\n","types,\n","brain\n","regions,\n","experimental\n","values,\n","units,\n","ion\n","currents,\n","channels,\n","and\n","conductances\n","and\n","model\n","organisms.\n","we\n","tested\n","a\n","traditional\n","rule-based\n","approach,\n","a\n","conditional\n","random\n","field\n","and\n","a\n","model\n","using\n","deep\n","learning\n","named\n","entity\n","recognition,\n","finding\n","that\n","the\n","deep\n","learning\n","model\n","was\n","superior.\n","our\n","final\n","results\n","show\n","that\n","we\n","can\n","detect\n","a\n","range\n","of\n","named\n","entities\n","of\n","interest\n","to\n","the\n","neuroscientist\n","with\n","a\n","macro\n","average\n","precision,\n","recall\n","and\n","f1\n","score\n","of\n","0.866,\n","0.817\n","and\n","0.837\n","respectively.\n","the\n","contributions\n","of\n","this\n","work\n","are\n","as\n","follows:\n","1)\n","we\n","provide\n","a\n","set\n","of\n","named\n","entity\n","recognition\n","(ner)\n","tools\n","that\n","are\n","capable\n","of\n","detecting\n","neuroscience\n","entities\n","with\n","performance\n","above\n","or\n","similar\n","to\n","prior\n","work.\n","2)\n","we\n","propose\n","a\n","methodology\n","for\n","training\n","ner\n","tools\n","for\n","neuroscience\n","that\n","requires\n","very\n","little\n","training\n","data\n","to\n","get\n","strong\n","performance.\n","this\n","can\n","be\n","adapted\n","for\n","any\n","sub-domain\n","within\n","neuroscience.\n","3)\n","we\n","provide\n","a\n","small\n","corpus\n","with\n","annotations\n","for\n","multiple\n","entity\n","types,\n","as\n","well\n","as\n","annotation\n","guidelines\n","to\n","help\n","others\n","reproduce\n","our\n","experiments.\n"]}]},{"cell_type":"code","metadata":{"id":"JzDhNIJgDxme","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761830945256,"user_tz":240,"elapsed":529,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"e48f801a-3100-4c05-cbb0-ea46136fe34c"},"source":["from nltk import word_tokenize\n","import nltk\n","\n","# Download the required resource if not already present\n","try:\n","    nltk.data.find('tokenizers/punkt_tab/english/')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","\n","\n","words_nltk = word_tokenize(document_lower)\n","\n","for word in words_nltk:\n","  print(word)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["the\n","curation\n","of\n","neuroscience\n","entities\n","is\n","crucial\n","to\n","ongoing\n","efforts\n","in\n","neuroinformatics\n","and\n","computational\n","neuroscience\n",",\n","such\n","as\n","those\n","being\n","deployed\n","in\n","the\n","context\n","of\n","continuing\n","large-scale\n","brain\n","modelling\n","projects\n",".\n","however\n",",\n","manually\n","sifting\n","through\n","thousands\n","of\n","articles\n","for\n","new\n","information\n","about\n","modelled\n","entities\n","is\n","a\n","painstaking\n","and\n","low-reward\n","task\n",".\n","text\n","mining\n","can\n","be\n","used\n","to\n","help\n","a\n","curator\n","extract\n","relevant\n","information\n","from\n","this\n","literature\n","in\n","a\n","systematic\n","way\n",".\n","we\n","propose\n","the\n","application\n","of\n","text\n","mining\n","methods\n","for\n","the\n","neuroscience\n","literature\n",".\n","specifically\n",",\n","two\n","computational\n","neuroscientists\n","annotated\n","a\n","corpus\n","of\n","entities\n","pertinent\n","to\n","neuroscience\n","using\n","active\n","learning\n","techniques\n","to\n","enable\n","swift\n",",\n","targeted\n","annotation\n",".\n","we\n","then\n","trained\n","machine\n","learning\n","models\n","to\n","recognise\n","the\n","entities\n","that\n","have\n","been\n","identified\n",".\n","the\n","entities\n","covered\n","are\n","neuron\n","types\n",",\n","brain\n","regions\n",",\n","experimental\n","values\n",",\n","units\n",",\n","ion\n","currents\n",",\n","channels\n",",\n","and\n","conductances\n","and\n","model\n","organisms\n",".\n","we\n","tested\n","a\n","traditional\n","rule-based\n","approach\n",",\n","a\n","conditional\n","random\n","field\n","and\n","a\n","model\n","using\n","deep\n","learning\n","named\n","entity\n","recognition\n",",\n","finding\n","that\n","the\n","deep\n","learning\n","model\n","was\n","superior\n",".\n","our\n","final\n","results\n","show\n","that\n","we\n","can\n","detect\n","a\n","range\n","of\n","named\n","entities\n","of\n","interest\n","to\n","the\n","neuroscientist\n","with\n","a\n","macro\n","average\n","precision\n",",\n","recall\n","and\n","f1\n","score\n","of\n","0.866\n",",\n","0.817\n","and\n","0.837\n","respectively\n",".\n","the\n","contributions\n","of\n","this\n","work\n","are\n","as\n","follows\n",":\n","1\n",")\n","we\n","provide\n","a\n","set\n","of\n","named\n","entity\n","recognition\n","(\n","ner\n",")\n","tools\n","that\n","are\n","capable\n","of\n","detecting\n","neuroscience\n","entities\n","with\n","performance\n","above\n","or\n","similar\n","to\n","prior\n","work\n",".\n","2\n",")\n","we\n","propose\n","a\n","methodology\n","for\n","training\n","ner\n","tools\n","for\n","neuroscience\n","that\n","requires\n","very\n","little\n","training\n","data\n","to\n","get\n","strong\n","performance\n",".\n","this\n","can\n","be\n","adapted\n","for\n","any\n","sub-domain\n","within\n","neuroscience\n",".\n","3\n",")\n","we\n","provide\n","a\n","small\n","corpus\n","with\n","annotations\n","for\n","multiple\n","entity\n","types\n",",\n","as\n","well\n","as\n","annotation\n","guidelines\n","to\n","help\n","others\n","reproduce\n","our\n","experiments\n",".\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"RcffYOjHGPly"},"source":["# Sentence segmentation"]},{"cell_type":"code","metadata":{"id":"cQ-MAWavGX--","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761831091965,"user_tz":240,"elapsed":60,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"c89781cd-c31c-47f6-af80-0e6ea40ae6bf"},"source":["#similarly, a simply way to split sentence is to split by '.'\n","sentences_split = document_lower.split('.')\n","count = 0\n","for sentence in sentences_split:\n","  print('sentence', count, ':', sentence)\n","  count += 1\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence 0 : the curation of neuroscience entities is crucial to ongoing efforts in neuroinformatics and computational neuroscience, such as those being deployed in the context of continuing large-scale brain modelling projects\n","sentence 1 :  however, manually sifting through thousands of articles for new information about modelled entities is a painstaking and low-reward task\n","sentence 2 :  text mining can be used to help a curator extract relevant information from this literature in a systematic way\n","sentence 3 :  we propose the application of text mining methods for the neuroscience literature\n","sentence 4 :  specifically, two computational neuroscientists annotated a corpus of entities pertinent to neuroscience using active learning techniques to enable swift, targeted annotation\n","sentence 5 :  we then trained machine learning models to recognise the entities that have been identified\n","sentence 6 :  the entities covered are neuron types, brain regions, experimental values, units, ion currents, channels, and conductances and model organisms\n","sentence 7 :  we tested a traditional rule-based approach, a conditional random field and a model using deep learning named entity recognition, finding that the deep learning model was superior\n","sentence 8 :  our final results show that we can detect a range of named entities of interest to the neuroscientist with a macro average precision, recall and f1 score of 0\n","sentence 9 : 866, 0\n","sentence 10 : 817 and 0\n","sentence 11 : 837 respectively\n","sentence 12 :  the contributions of this work are as follows: 1) we provide a set of named entity recognition (ner) tools that are capable of detecting neuroscience entities with performance above or similar to prior work\n","sentence 13 :  2) we propose a methodology for training ner tools for neuroscience that requires very little training data to get strong performance\n","sentence 14 :  this can be adapted for any sub-domain within neuroscience\n","sentence 15 :  3) we provide a small corpus with annotations for multiple entity types, as well as annotation guidelines to help others reproduce our experiments\n","sentence 16 : \n"]}]},{"cell_type":"code","metadata":{"id":"aYEnqk3lHEy_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761831282507,"user_tz":240,"elapsed":69,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"0e2ab8b1-6cac-49ad-cede-c0ae30605d65"},"source":["from nltk import sent_tokenize\n","sentences_nltk = sent_tokenize(document_lower)\n","count = 0\n","for sentence in sentences_nltk:\n","  print('sentence', count, ':', sentence)\n","  count += 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence 0 : the curation of neuroscience entities is crucial to ongoing efforts in neuroinformatics and computational neuroscience, such as those being deployed in the context of continuing large-scale brain modelling projects.\n","sentence 1 : however, manually sifting through thousands of articles for new information about modelled entities is a painstaking and low-reward task.\n","sentence 2 : text mining can be used to help a curator extract relevant information from this literature in a systematic way.\n","sentence 3 : we propose the application of text mining methods for the neuroscience literature.\n","sentence 4 : specifically, two computational neuroscientists annotated a corpus of entities pertinent to neuroscience using active learning techniques to enable swift, targeted annotation.\n","sentence 5 : we then trained machine learning models to recognise the entities that have been identified.\n","sentence 6 : the entities covered are neuron types, brain regions, experimental values, units, ion currents, channels, and conductances and model organisms.\n","sentence 7 : we tested a traditional rule-based approach, a conditional random field and a model using deep learning named entity recognition, finding that the deep learning model was superior.\n","sentence 8 : our final results show that we can detect a range of named entities of interest to the neuroscientist with a macro average precision, recall and f1 score of 0.866, 0.817 and 0.837 respectively.\n","sentence 9 : the contributions of this work are as follows: 1) we provide a set of named entity recognition (ner) tools that are capable of detecting neuroscience entities with performance above or similar to prior work.\n","sentence 10 : 2) we propose a methodology for training ner tools for neuroscience that requires very little training data to get strong performance.\n","sentence 11 : this can be adapted for any sub-domain within neuroscience.\n","sentence 12 : 3) we provide a small corpus with annotations for multiple entity types, as well as annotation guidelines to help others reproduce our experiments.\n"]}]},{"cell_type":"markdown","metadata":{"id":"_QdJNhfcl4Qo"},"source":["From now, we can have a very simple case study on analyzing word frequency of the document which shows the impact of text processing methods"]},{"cell_type":"markdown","metadata":{"id":"JsPlLv4iereb"},"source":["# Word frequency analysis"]},{"cell_type":"markdown","metadata":{"id":"Z7WYDOkJYl1v"},"source":["# **Exercise 1**"]},{"cell_type":"code","metadata":{"id":"CfUetwCLevZd"},"source":["### Exercise: complete the function that takes a list of words as an input and\n","### produce a dictionary contains each word and its frequency\n","### For each word in the document, we want to keep track its frequency in a python dictory\n","### Please fill in the code blocks and finish the function create_word_freq()\n","\n","def create_word_freq(words):\n","    #step 1 create a dictionary called word_freq\n","    #write your codes here\n","    word_freq = {}\n","\n","    #step 2: iterate each word in words, keep track of its frequency\n","    #the code should start with something like: for word in words:\n","    #write your codes here\n","\n","    for word in words:\n","\n","        if word not in word_freq:\n","          #write your codes here\n","          word_freq[word] = 1\n","        else:\n","          #write your codes here\n","          word_freq[word] += 1\n","\n","    #step 3: return the dictionary\n","    #write your codes here\n","    return word_freq\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLCFqN01Qix-","executionInfo":{"status":"ok","timestamp":1761833052489,"user_tz":240,"elapsed":25,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"9961161a-df2d-4d12-ac58-0c68b685be6f"},"source":["#now we can use our function\n","\n","#first, let's tokenize the document\n","words_nltk = word_tokenize(document)\n","\n","#now let's call the function that we just created\n","word_freq = create_word_freq(words_nltk)\n","\n","print(word_freq)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'The': 3, 'curation': 1, 'of': 11, 'neuroscience': 7, 'entities': 7, 'is': 2, 'crucial': 1, 'to': 9, 'ongoing': 1, 'efforts': 1, 'in': 3, 'neuroinformatics': 1, 'and': 7, 'computational': 2, ',': 15, 'such': 1, 'as': 4, 'those': 1, 'being': 1, 'deployed': 1, 'the': 6, 'context': 1, 'continuing': 1, 'large-scale': 1, 'brain': 1, 'modelling': 1, 'projects': 1, '.': 13, 'However': 1, 'manually': 1, 'sifting': 1, 'through': 1, 'thousands': 1, 'articles': 1, 'for': 6, 'new': 1, 'information': 2, 'about': 1, 'modelled': 1, 'a': 12, 'painstaking': 1, 'low-reward': 1, 'task': 1, 'Text': 1, 'mining': 2, 'can': 3, 'be': 2, 'used': 1, 'help': 2, 'curator': 1, 'extract': 1, 'relevant': 1, 'from': 1, 'this': 2, 'literature': 2, 'systematic': 1, 'way': 1, 'We': 6, 'propose': 2, 'application': 1, 'text': 1, 'methods': 1, 'Specifically': 1, 'two': 1, 'neuroscientists': 1, 'annotated': 1, 'corpus': 2, 'pertinent': 1, 'using': 2, 'active': 1, 'learning': 4, 'techniques': 1, 'enable': 1, 'swift': 1, 'targeted': 1, 'annotation': 2, 'then': 1, 'trained': 1, 'machine': 1, 'models': 1, 'recognise': 1, 'that': 5, 'have': 1, 'been': 1, 'identified': 1, 'covered': 1, 'are': 3, 'Neuron': 1, 'Types': 1, 'Brain': 1, 'Regions': 1, 'Experimental': 1, 'Values': 1, 'Units': 1, 'Ion': 1, 'Currents': 1, 'Channels': 1, 'Conductances': 1, 'Model': 1, 'organisms': 1, 'tested': 1, 'traditional': 1, 'rule-based': 1, 'approach': 1, 'conditional': 1, 'random': 1, 'field': 1, 'model': 2, 'deep': 2, 'named': 2, 'entity': 2, 'recognition': 1, 'finding': 1, 'was': 1, 'superior': 1, 'Our': 1, 'final': 1, 'results': 1, 'show': 1, 'we': 1, 'detect': 1, 'range': 1, 'interest': 1, 'neuroscientist': 1, 'with': 3, 'macro': 1, 'average': 1, 'precision': 1, 'recall': 1, 'F1': 1, 'score': 1, '0.866': 1, '0.817': 1, '0.837': 1, 'respectively': 1, 'contributions': 1, 'work': 2, 'follows': 1, ':': 1, '1': 1, ')': 4, 'provide': 2, 'set': 1, 'Named': 1, 'Entity': 1, 'Recognition': 1, '(': 1, 'NER': 2, 'tools': 2, 'capable': 1, 'detecting': 1, 'performance': 2, 'above': 1, 'or': 1, 'similar': 1, 'prior': 1, '2': 1, 'methodology': 1, 'training': 2, 'requires': 1, 'very': 1, 'little': 1, 'data': 1, 'get': 1, 'strong': 1, 'This': 1, 'adapted': 1, 'any': 1, 'sub-domain': 1, 'within': 1, '3': 1, 'small': 1, 'annotations': 1, 'multiple': 1, 'types': 1, 'well': 1, 'guidelines': 1, 'others': 1, 'reproduce': 1, 'our': 1, 'experiments': 1}\n"]}]},{"cell_type":"code","metadata":{"id":"z6lI7Hn9dSVD"},"source":["#let's create a function which prints top k most frequent words based on the freq dict\n","#this will help us explore word frequency more efficiently\n","from operator import itemgetter\n","def print_top_freq_words(word_freq_dict, k):\n","    freqs = sorted(word_freq_dict.items(), key=itemgetter(1), reverse=True)\n","    print('top', k, 'terms')\n","    for freq in freqs[:k]:\n","        print(freq)\n","    print('total number of unique tokens', len(word_freq_dict))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EopEx3naewBK","executionInfo":{"status":"ok","timestamp":1761833412970,"user_tz":240,"elapsed":15,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"87bc44cf-8620-4ea8-94a0-c8645c807cb0"},"source":["#now let's print top 20 tokens for our word freq dict\n","print_top_freq_words(word_freq, 20)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["top 20 terms\n","(',', 15)\n","('.', 13)\n","('a', 12)\n","('of', 11)\n","('to', 9)\n","('neuroscience', 7)\n","('entities', 7)\n","('and', 7)\n","('the', 6)\n","('for', 6)\n","('We', 6)\n","('that', 5)\n","('as', 4)\n","('learning', 4)\n","(')', 4)\n","('The', 3)\n","('in', 3)\n","('can', 3)\n","('are', 3)\n","('with', 3)\n","total number of unique tokens 181\n"]}]},{"cell_type":"markdown","metadata":{"id":"M8akbTgUSX2p"},"source":["Question: please have a look at top 20 tokens. Do you think these tokens are meaningful?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i5SyCX0ygx7z"},"source":["# Removing puntuations"]},{"cell_type":"code","metadata":{"id":"clHp9cVPhxvQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761834129734,"user_tz":240,"elapsed":9,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"032ad84b-a567-4c8a-d4e6-64357989099b"},"source":["from string import punctuation\n","\n","print(punctuation)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDtY5bfjT44v","executionInfo":{"status":"ok","timestamp":1761834189448,"user_tz":240,"elapsed":18,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"68194dbc-ba1d-4409-907e-ae36cf813b90"},"source":["# We can use the if statement to check whether a word is one of the punctuations\n","word_a = '.'\n","if word_a in punctuation:\n","  print(word_a, 'is a punctuation')\n","else:\n","  print(word_a, 'is not a punctuation')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[". is a punctuation\n"]}]},{"cell_type":"markdown","metadata":{"id":"LIKw43MiYOjx"},"source":["# **Exercise 2**"]},{"cell_type":"code","metadata":{"id":"5XOkS6YPTUau"},"source":["### Exercise: now that we have learnt a few text processing methods\n","### Let's write one function that applies these method to curate text\n","### It takes a document (i.e., a string) as the input, performs text processing, and produces a list of tokens\n","\n","def preprocess_text(document):\n","\n","    #step 1: tokenize the document\n","    #write your codes here\n","    words_nltk = word_tokenize(document)\n","    #let's create a list to track the final tokens\n","    words_final = []\n","    #step 2: for each token, check whether it is a punctuation\n","    #if a token is not a punctuation, make it in lower case and add it to words_final\n","    #write your codes here\n","    for word in words_nltk:\n","        #if the word is not a punct, lower its case\n","        #and append to the list\n","        if word not in punctuation:\n","          #lower the case\n","          #write your codes here\n","          #append to the list\n","          #write your codes here\n","          words_final.append(word.lower())\n","        else:\n","          continue\n","    #step 3: return the final list of tokens\n","    #write your codes here\n","    return words_final\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0BfA1qzcVEy","executionInfo":{"status":"ok","timestamp":1761834279539,"user_tz":240,"elapsed":20,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"3b52cd42-849b-4693-a619-7dfce27a905b"},"source":["#now let's apply the method to the document\n","word_tokens = preprocess_text(document)\n","\n","#let's construct the word frequency dict and print the top frequent terms again\n","word_freq = create_word_freq(word_tokens)\n","print_top_freq_words(word_freq, 20)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["top 20 terms\n","('a', 12)\n","('of', 11)\n","('the', 9)\n","('to', 9)\n","('neuroscience', 7)\n","('entities', 7)\n","('and', 7)\n","('we', 7)\n","('for', 6)\n","('that', 5)\n","('as', 4)\n","('learning', 4)\n","('in', 3)\n","('can', 3)\n","('this', 3)\n","('are', 3)\n","('model', 3)\n","('named', 3)\n","('entity', 3)\n","('with', 3)\n","total number of unique tokens 165\n"]}]},{"cell_type":"markdown","metadata":{"id":"DtT-DEFOgPxs"},"source":["Question: compare with the previous top 20 words, is it better? Are these words meaningful?"]},{"cell_type":"markdown","metadata":{"id":"yhOpooN3jzU5"},"source":["# Removing stopwords"]},{"cell_type":"code","metadata":{"id":"0zsNCas7j3ZF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761834626951,"user_tz":240,"elapsed":61,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"a0aec637-8d2b-4758-b12c-e5daed11f879"},"source":["from nltk.corpus import stopwords\n","stopword_list = set(stopwords.words('english'))\n","print('number of stopword list', len(stopword_list))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of stopword list 198\n"]}]},{"cell_type":"code","metadata":{"id":"DPIyk8slc-UG"},"source":["#word_a = 'text'\n","\n","#if word_a in stopword_list:"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6rZYtB6gova"},"source":["# **Exercise 3**"]},{"cell_type":"code","metadata":{"id":"U2NjKFdpgpLR"},"source":["### Exercise: let's update our text processing method by removing stopwords\n","\n","def preprocess_text(document):\n","\n","    #step 1: tokenize the document\n","    #write your codes here\n","    words_nltk = word_tokenize(document)\n","    #let's create a list to track the final tokens\n","    words_final = []\n","    #step 2: for each token, check whether it is a punctuation\n","    #if a token is not a punctuation and not a stopword (NEW), add its lower case to words_final\n","    #write your codes here\n","    # for word in words_nltk:\n","    #     if word in stopword_list:\n","    #       continue\n","    #     elif word not in punctuation:\n","    #       #lower the case\n","    #       #write your codes here\n","    #       #append to the list\n","    #       #write your codes here\n","    #       words_final.append(word.lower())\n","    #     else:\n","    #       continue\n","    for word in words_nltk:\n","      if word not in punctuation and word not in stopword_list:\n","        words_final.append(word.lower())\n","\n","    #step 3: return the final list of tokens\n","    #write your codes here\n","    return words_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2QcO81ShaWZ","executionInfo":{"status":"ok","timestamp":1761834917349,"user_tz":240,"elapsed":16,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"67d2bd89-60f3-42bf-fe26-cfb3533ffc72"},"source":["#now let's apply the updated method to the document again and see top k tokens\n","word_tokens = preprocess_text(document)\n","\n","#let's construct the word frequency dict and print the top frequent terms again\n","word_freq = create_word_freq(word_tokens)\n","print_top_freq_words(word_freq, 20)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["top 20 terms\n","('neuroscience', 7)\n","('entities', 7)\n","('we', 6)\n","('learning', 4)\n","('the', 3)\n","('model', 3)\n","('named', 3)\n","('entity', 3)\n","('computational', 2)\n","('brain', 2)\n","('information', 2)\n","('text', 2)\n","('mining', 2)\n","('help', 2)\n","('literature', 2)\n","('propose', 2)\n","('corpus', 2)\n","('using', 2)\n","('annotation', 2)\n","('types', 2)\n","total number of unique tokens 138\n"]}]},{"cell_type":"markdown","metadata":{"id":"RQowSORYpop1"},"source":["Question: are the top 20 terms better than previous versions and more informative?"]},{"cell_type":"markdown","metadata":{"id":"AEVZIh6Z9nmh"},"source":["# Pos-tagging"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sv58yCt59nCF","executionInfo":{"status":"ok","timestamp":1761835002515,"user_tz":240,"elapsed":1062,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"6086d983-a0e1-4c12-c300-d970c2928de9"},"source":["# Pos-tagging\n","import nltk\n","try:\n","    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n","except LookupError:\n","    nltk.download('averaged_perceptron_tagger_eng')\n","\n","\n","example_sentence = 'we are demonstrating an example of pos-tagging'\n","example_pos = nltk.pos_tag(nltk.word_tokenize(example_sentence))\n","print(example_pos)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('we', 'PRP'), ('are', 'VBP'), ('demonstrating', 'VBG'), ('an', 'DT'), ('example', 'NN'), ('of', 'IN'), ('pos-tagging', 'NN')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"N5SyjJY3pNc0"},"source":["# Stemming and lemmatization"]},{"cell_type":"code","metadata":{"id":"iRionN8dpQql","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761835928515,"user_tz":240,"elapsed":6,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"faf30b7a-ed96-4c23-9812-97e306b6b8b0"},"source":["#an example of using stemming\n","from nltk.stem import PorterStemmer\n","stemmer = PorterStemmer()\n","\n","example_words = ['demonstrate', 'demonstrated', 'demonstration']\n","\n","for example_word in example_words:\n","  print(example_word, 'is stemmed to', stemmer.stem(example_word))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["demonstrate is stemmed to demonstr\n","demonstrated is stemmed to demonstr\n","demonstration is stemmed to demonstr\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNDK2Lt4meO8","executionInfo":{"status":"ok","timestamp":1761836378452,"user_tz":240,"elapsed":14,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"e63b7a5a-6fce-43a5-9908-cc2222fa704c"},"source":["#an example of using lemmatization\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","for example_word in example_words:\n","  print(example_word, 'is lemmatized to', lemmatizer.lemmatize(example_word))\n","\n","#lemmatization only works effectively with pos-tagging.\n","#let's see the next example"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["demonstrate is lemmatized to demonstrate\n","demonstrated is lemmatized to demonstrated\n","demonstration is lemmatized to demonstration\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmxO0ckumjAq","executionInfo":{"status":"ok","timestamp":1761836394449,"user_tz":240,"elapsed":25,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"450f49d4-3abe-448a-e1b8-36c76480d3ec"},"source":["example_sentence = 'we are demonstrating examples of lemmatization'\n","example_tokens = nltk.word_tokenize(example_sentence)\n","#[('we', 'pron'), ('are', 'verb')...]\n","example_pos = nltk.pos_tag(example_tokens)\n","\n","#map pos-tags to required categories of NLTK\n","def find_pos(tag):\n","  if tag.startswith('J'):\n","      return wordnet.ADJ\n","  elif tag.startswith('V'):\n","      return wordnet.VERB\n","  elif tag.startswith('N'):\n","      return wordnet.NOUN\n","  elif tag.startswith('R'):\n","      return wordnet.ADV\n","  else:\n","      return wordnet.NOUN\n","\n","for word, pos_tag in example_pos:\n","  word_tag = find_pos(pos_tag)\n","  print(word, 'has a pos tag of', pos_tag, 'is lemmatized to', lemmatizer.lemmatize(word, pos=word_tag))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["we has a pos tag of PRP is lemmatized to we\n","are has a pos tag of VBP is lemmatized to be\n","demonstrating has a pos tag of VBG is lemmatized to demonstrate\n","examples has a pos tag of NNS is lemmatized to example\n","of has a pos tag of IN is lemmatized to of\n","lemmatization has a pos tag of NN is lemmatized to lemmatization\n"]}]},{"cell_type":"markdown","metadata":{"id":"TSxCVksuHC2X"},"source":["# **Exercise 4**\n"]},{"cell_type":"code","metadata":{"id":"VrNPjFhpHLBy"},"source":["#now let's try the final exercise of the text processing method by adding stemming\n","#to our preprocess function\n","from nltk.stem import PorterStemmer\n","stemmer = PorterStemmer()\n","\n","def preprocess_text(document):\n","\n","    #step 1: tokenize the document\n","    #write your codes here\n","    words_nltk = word_tokenize(document)\n","\n","    #let's create a list to track the final tokens\n","    words_final = []\n","    #step 2: for each token, check whether it is a punctuation\n","    #if a token is not a punctuation and not a stopword, stem it (NEW) and add its lower case to words_final\n","    #write your codes here\n","    for word in words_nltk:\n","        if word not in punctuation and word not in stopword_list:\n","            #stem the word\n","            stemmed_word = stemmer.stem(word.lower())\n","            words_final.append(stemmed_word)\n","\n","    #step 3: return the final list of tokens\n","    #write your codes here\n","    return words_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vlZRempI-a_","executionInfo":{"status":"ok","timestamp":1761836418153,"user_tz":240,"elapsed":5,"user":{"displayName":"Blindside","userId":"07630539884879584070"}},"outputId":"719e10bc-4093-4aa8-eef2-c63500903e3e"},"source":["#now let's apply the updated method to the document again and see top k tokens\n","word_tokens = preprocess_text(document)\n","\n","#let's construct the word frequency dict and print the top frequent terms again\n","word_freq = create_word_freq(word_tokens)\n","print_top_freq_words(word_freq, 20)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["top 20 terms\n","('entiti', 10)\n","('neurosci', 7)\n","('model', 6)\n","('we', 6)\n","('annot', 4)\n","('learn', 4)\n","('the', 3)\n","('use', 3)\n","('train', 3)\n","('name', 3)\n","('curat', 2)\n","('comput', 2)\n","('brain', 2)\n","('inform', 2)\n","('text', 2)\n","('mine', 2)\n","('help', 2)\n","('literatur', 2)\n","('propos', 2)\n","('neuroscientist', 2)\n","total number of unique tokens 127\n"]}]},{"cell_type":"markdown","metadata":{"id":"oI7_CA4pJGOs"},"source":["Question: how many unique tokens are reduced after stemming?"]}]}