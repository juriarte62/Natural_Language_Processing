{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uhY0mytqw1W4"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"glYjzHaO15Wr"},"source":["In this notebook, we demonstrate how to train a multi-layer perceptron model for sentence similarity.\n","\n","Continuting with the last week, we use the dataset consisting of sentence pairs from PubMed articles (https://www.ncbi.nlm.nih.gov/pubmed/28881973). The input is a pair of sentences and the output is the similarity between the pair annotated by expert curators.\n","\n","We will cover the following:\n","\n","(1) A quick revision on what we have done last week\n","\n","(2) Feature engineering: implementing new features\n","\n","(3) Train an updated linear regression model\n","\n","(4) Train a MLP model\n","\n","(5) Hyperparameter tuning\n","\n","Note that training a deep learning model requires significant computational resources and time. Here we use a simple MLP model with a super small dataset for demonstration. In practice, it needs much larger datasets to train a robust deep learning model.\n"]},{"cell_type":"markdown","metadata":{"id":"pW_VHEXK3aK0"},"source":["# Install required libraries and load the dataset"]},{"cell_type":"code","metadata":{"id":"ghqnIJ9f1R48","outputId":"caa147de-af8e-4e34-ec06-da0a6761f234","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk import word_tokenize\n","from string import punctuation\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stopword_list = set(stopwords.words('english'))\n","from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B7I102xKyR3P","outputId":"940ba425-f649-4451-c259-cf46ddd5c765","colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["train_df = pd.read_csv('train-0.csv', sep='\\t', names=['sentence_1', 'sentence_2', 'similarity'])\n","test_df = pd.read_csv('test-0.csv', sep='\\t', names=['sentence_1', 'sentence_2', 'similarity'])\n","\n","print(train_df.head())\n","print(test_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                          sentence_1  ... similarity\n","0  This form of necrosis, also termed necroptosis...  ...        4.0\n","1  BAF53 and β-actin subunits have been implicate...  ...        3.8\n","2  T47D, MCF-7, Skbr3, HeLa, and Caco-2 cells wer...  ...        3.0\n","3  This oxidative branch activity is elevated in ...  ...        0.0\n","4  Centrosomes increase both in size and in micro...  ...        0.0\n","\n","[5 rows x 3 columns]\n","                                          sentence_1  ... similarity\n","0  It has been shown, however, that ubiquitinatio...  ...        3.0\n","1  Ironically, Rest has recently been described a...  ...        3.2\n","2  In PC9 cells, loss of GATA6 and/or HOPX did no...  ...        0.2\n","3  MiR-223 seems to be a target molecule of TFs r...  ...        1.2\n","4  BAF53 and β-actin subunits have been implicate...  ...        3.0\n","\n","[5 rows x 3 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vQqYoS8S3dwE"},"source":["# Feature engineering"]},{"cell_type":"markdown","metadata":{"id":"bDT2rX-I3gC9"},"source":["We need to think about what features are important for sentence similarity. One assumption can be made the similarity between the sentences will be higher if they share more words in common. So at the first step, we could use the jaccard similarity (the number of common words in a pair over the number of total words in a pair) as a feature.\n","\n","For demonstration, we use this feature only and train a linear regression model. But there are many features beyond. For example: whether the pairs talk about the same entities?, whether the pairs have common phrases?, and whether the pairs come from the same sections of the paper?\n","\n","You could think about many features and train the models again. The method and logic remain the same.\n","\n","NEW: we add two more new features: (1) the size ratio and (2) the edit distance, e.g., the distance between 'feature' and 'features' is 1"]},{"cell_type":"markdown","metadata":{"id":"JFNEA9_65OHC"},"source":["# Preprocess sentences"]},{"cell_type":"markdown","metadata":{"id":"JwuYjbEy3z7r"},"source":["To generate the feature, we firstly need to pre-porcess the sentences. Think about the preprocessing steps that you have learnt in week 3 and decide which to apply. For demonstration, I used case folding, word tokenization, stop words removal and punctuations removal. You can try others to experiment."]},{"cell_type":"code","metadata":{"id":"GJLcWpfF0Cd-"},"source":["def preprocess_sentence(sentence):\n","  #case folding\n","  sentence = sentence.lower()\n","  processed_tokens = []\n","  #word tokenization\n","  for token in word_tokenize(sentence):\n","    #remove stopwords and punctuations\n","    if token not in punctuation and token not in stopword_list:\n","      processed_tokens.append(token)\n","  return processed_tokens\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbWWhEB35rkR"},"source":["After creating the preprocess sentence function, we can compute the number of shared words for each pair"]},{"cell_type":"code","metadata":{"id":"YKIBzgRF5xzc"},"source":["def compute_shared_words(sentence1, sentence2):\n","  #preprocess the pairs using our created preprocess functions\n","  sentence1_tokens = preprocess_sentence(sentence1)\n","  sentence2_tokens = preprocess_sentence(sentence2)\n","  #use python in-built function to calculate the overlap and union between the list\n","  shared_words = len(list(set(sentence1_tokens) & set(sentence2_tokens)))\n","  total_words = len(list(set(sentence1_tokens) | set(sentence2_tokens)))\n","  #jaccard similarity = shared_words/total_words\n","  #print(sentence1, sentence2, shared_words/total_words)\n","  return shared_words/total_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNR1JR2ox_zS"},"source":["Add  new features. You could try other features as well."]},{"cell_type":"code","metadata":{"id":"ZUci40C3f-27"},"source":["def compute_length_ratio(sentence1, sentence2):\n","  sentence1_tokens = preprocess_sentence(sentence1)\n","  sentence2_tokens = preprocess_sentence(sentence2)\n","  if len(sentence1_tokens) <= len(sentence2_tokens):\n","    return len(sentence1_tokens)/len(sentence2_tokens)\n","  else:\n","    return len(sentence2_tokens)/len(sentence1_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muIN9f3Zo0Sk"},"source":["def computed_shared_ngrams(sentence1, sentence2, n):\n","  sentence1_tokens = preprocess_sentence(sentence1)\n","  sentence2_tokens = preprocess_sentence(sentence2)\n","\n","  sentence1_ngrams = set(nltk.ngrams(' '.join(sentence1_tokens), n))\n","  sentence2_ngrams = set(nltk.ngrams(' '.join(sentence2_tokens), n))\n","  return 1 - nltk.jaccard_distance(sentence1_ngrams, sentence2_ngrams)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tf2DRV0Di8jp"},"source":["def compute_edit_distance(sentence1, sentence2):\n","  sentence1_tokens = preprocess_sentence(sentence1)\n","  sentence2_tokens = preprocess_sentence(sentence2)\n","  return nltk.edit_distance(' '.join(sentence1_tokens), ' '.join(sentence2_tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cK0e_cSs670z"},"source":["Now we can go ahead to compute the features for training set and testing set"]},{"cell_type":"code","metadata":{"id":"jrgzUfr_7DJD"},"source":["def compute_features(instances):\n","  features = []\n","  #go through each row of the dataframe\n","  for _, row in instances.iterrows():\n","    row_feature = []\n","    #compute the three features\n","    row_feature.append(compute_shared_words(row.sentence_1, row.sentence_2))\n","    row_feature.append(compute_length_ratio(row.sentence_1, row.sentence_2))\n","    row_feature.append(compute_edit_distance(row.sentence_1, row.sentence_2))\n","    row_feature.append(computed_shared_ngrams(row.sentence_1, row.sentence_2, 2))\n","    row_feature.append(computed_shared_ngrams(row.sentence_1, row.sentence_2, 3))\n","    features.append(np.array(row_feature))\n","  return np.array(features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQlAI6xk7il8","outputId":"b9880423-036f-42e8-fd3e-1e1806b1b1f5","colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["train_features = compute_features(train_df)\n","test_features = compute_features(test_df)\n","\n","print(train_features.shape)\n","print(test_features.shape)\n","\n","#train the linear regression model using the training data\n","reg = LinearRegression().fit(train_features, train_df['similarity'])\n","\n","predictions = reg.predict(test_features)\n","\n","print(predictions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(80, 5)\n","(20, 5)\n","[2.35132634 2.10961077 1.00882604 1.14717013 2.08288977 1.99332065\n"," 2.42909151 3.0762702  3.90012101 1.25958376 1.28381563 1.57025241\n"," 1.29908133 1.11894975 1.01228613 2.13004045 0.89101831 2.50396893\n"," 1.69758698 2.13110393]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JVZhLkdJ92U8"},"source":["# Evaluation"]},{"cell_type":"markdown","metadata":{"id":"g6gHhzTx94_b"},"source":["Person correlation is used to evaluate the performance of the model, suggested by the dataset creators. It ranges from 0 to 1. The higher value indicates it is more similar to human annotations."]},{"cell_type":"code","metadata":{"id":"smndjKV1-BEb","outputId":"1e607854-d8f7-4216-c965-7accd664910a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from scipy.stats import pearsonr\n","\n","print (pearsonr(predictions, test_df['similarity'])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7788680664542982\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oONUZySMyO_o"},"source":["# Train a MLP model"]},{"cell_type":"code","metadata":{"id":"hn79xZkvsCHk","outputId":"fdade150-881d-46aa-c2e9-a1ff1d5ecb07","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.neural_network import MLPRegressor\n","\n","mlp = MLPRegressor(hidden_layer_sizes=(100), max_iter=1000,\n","                   batch_size=20, random_state=0).fit(train_features, train_df['similarity'])\n","\n","predictions = mlp.predict(test_features)\n","\n","print (pearsonr(predictions, test_df['similarity'])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6009781080964407\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3PXlRVO1yq9W"},"source":["# Hyperparameter tuning"]},{"cell_type":"code","metadata":{"id":"3Qf5FYHOlBfy","outputId":"10f5c403-602f-4c98-f2db-b5bb8fabb0c5","colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["for hidden_units in range (10, 201, 10):\n","  mlp = MLPRegressor(hidden_layer_sizes=(hidden_units), max_iter=1000,\n","                   batch_size=20, random_state=0).fit(train_features, train_df['similarity'])\n","\n","  predictions = mlp.predict(test_features)\n","\n","  print (hidden_units, pearsonr(predictions, test_df['similarity'])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10 0.41477545400640803\n","20 0.7030378975884407\n","30 0.6127807298214164\n","40 0.6543617015861044\n","50 0.6901274190444908\n","60 0.695676701666488\n","70 0.6825270694290609\n","80 0.6542659764767693\n","90 0.6352368701930838\n","100 0.6009781080964407\n","110 0.649257720519443\n","120 0.6618048646929301\n","130 0.6547502396317846\n","140 0.6173160490748243\n","150 0.6933434647930015\n","160 0.6307066582205453\n","170 0.6566601112592119\n","180 0.6424480052268968\n","190 0.7213345501514812\n","200 0.6762522177537691\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c2SaJqFpyxZ4"},"source":["# Notes"]},{"cell_type":"markdown","metadata":{"id":"CaZRnReeyzV6"},"source":["We finished the first set of examples of using deep learning models for text mining. As mentioned, this is a demonstration for a simple MLP model with a super small dataset. However, the full pipeline holds and you are encouraged to apply to larger datasets and more challenging problems for practice."]}]}